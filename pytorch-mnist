{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5303a2d",
   "metadata": {
    "papermill": {
     "duration": 0.00455,
     "end_time": "2024-08-07T01:02:03.417756",
     "exception": false,
     "start_time": "2024-08-07T01:02:03.413206",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Digit Recognization w/ PyTorch\n",
    "\n",
    "### Using PyTorch for MNIST Classification\n",
    "Using PyTorch, we aim to classify photos of digits from 0-9. This is an introductory notebook and closely follows sentdexs PyTorch series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a915f075",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T01:02:03.428204Z",
     "iopub.status.busy": "2024-08-07T01:02:03.427731Z",
     "iopub.status.idle": "2024-08-07T01:02:10.262968Z",
     "shell.execute_reply": "2024-08-07T01:02:10.261667Z"
    },
    "papermill": {
     "duration": 6.843776,
     "end_time": "2024-08-07T01:02:10.265863",
     "exception": false,
     "start_time": "2024-08-07T01:02:03.422087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab00381",
   "metadata": {
    "papermill": {
     "duration": 0.004068,
     "end_time": "2024-08-07T01:02:10.274756",
     "exception": false,
     "start_time": "2024-08-07T01:02:10.270688",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Loading in the dataset\n",
    "\n",
    "datasets.MNIST from PyTorch expects data in a specific format. Here, I wanted to work with PyTorch DataLoader for batching/shuffling/other data loading features. This is unnecessary as you can just load the data in as normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7afb029",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T01:02:10.285184Z",
     "iopub.status.busy": "2024-08-07T01:02:10.284630Z",
     "iopub.status.idle": "2024-08-07T01:02:15.432995Z",
     "shell.execute_reply": "2024-08-07T01:02:15.431822Z"
    },
    "papermill": {
     "duration": 5.15653,
     "end_time": "2024-08-07T01:02:15.435577",
     "exception": false,
     "start_time": "2024-08-07T01:02:10.279047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, has_labels=True):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.has_labels = has_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.has_labels:\n",
    "            label = torch.tensor(self.data.iloc[idx, 0], dtype=torch.long)\n",
    "            image = self.data.iloc[idx, 1:].values.astype('uint8').reshape(28, 28)\n",
    "        else:\n",
    "            label = torch.tensor(-1, dtype=torch.long)  # Placeholder label\n",
    "            image = self.data.iloc[idx, :].values.astype('uint8').reshape(28, 28)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "    \n",
    "train_dataset = MNISTDataset('/kaggle/input/digit-recognizer/train.csv', transform=transform, has_labels=True)\n",
    "test_dataset = MNISTDataset('/kaggle/input/digit-recognizer/test.csv', transform=transform, has_labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ee3d43",
   "metadata": {
    "papermill": {
     "duration": 0.003923,
     "end_time": "2024-08-07T01:02:15.443835",
     "exception": false,
     "start_time": "2024-08-07T01:02:15.439912",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Creating the model\n",
    "\n",
    "In order to create the model, we need a class, which we'll call net. This class will inherit from nn.Module class. We'll define values for layers where fc stands for fully connected. To be fully connected means that every neuron will be fully connected to attaching neurons. \n",
    "\n",
    "The first layer takes in 28x28 because the images are 28x28 images so we expect a flattened array of 1x784 (28x28). The first layer will then output 64 connections, with the next layer taking in 64 (previous layer output) and then outputting another 64. The third layer does the same thing but fc4 will output 10 because we have 10 classes (0-9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1acf3ffc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T01:02:15.453873Z",
     "iopub.status.busy": "2024-08-07T01:02:15.453487Z",
     "iopub.status.idle": "2024-08-07T01:02:15.493492Z",
     "shell.execute_reply": "2024-08-07T01:02:15.492295Z"
    },
    "papermill": {
     "duration": 0.048576,
     "end_time": "2024-08-07T01:02:15.496578",
     "exception": false,
     "start_time": "2024-08-07T01:02:15.448002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64,64)\n",
    "        self.fc4 = nn.Linear(64,10)\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a5de82",
   "metadata": {
    "papermill": {
     "duration": 0.003865,
     "end_time": "2024-08-07T01:02:15.504737",
     "exception": false,
     "start_time": "2024-08-07T01:02:15.500872",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Adding a forward method\n",
    "\n",
    "In our model, x is the parameter for the forward method, meaning that we need to pass it through the layers. By adding activation functions for the layers (relu), we can keep our data scaled between 0-1 rather than spiking in values.\n",
    "\n",
    "The softmax in the last line means that the outputs are a confidence score that sums up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caec9648",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T01:02:15.515259Z",
     "iopub.status.busy": "2024-08-07T01:02:15.514298Z",
     "iopub.status.idle": "2024-08-07T01:02:15.523702Z",
     "shell.execute_reply": "2024-08-07T01:02:15.522541Z"
    },
    "papermill": {
     "duration": 0.01704,
     "end_time": "2024-08-07T01:02:15.525892",
     "exception": false,
     "start_time": "2024-08-07T01:02:15.508852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64,64)\n",
    "        self.fc4 = nn.Linear(64,10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5140b7",
   "metadata": {
    "papermill": {
     "duration": 0.003972,
     "end_time": "2024-08-07T01:02:15.534163",
     "exception": false,
     "start_time": "2024-08-07T01:02:15.530191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here, we're creating some random image, flattening the image, and then passing it through our neural network. Looks like it works as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a18c8071",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T01:02:15.544596Z",
     "iopub.status.busy": "2024-08-07T01:02:15.543787Z",
     "iopub.status.idle": "2024-08-07T01:02:15.649618Z",
     "shell.execute_reply": "2024-08-07T01:02:15.648466Z"
    },
    "papermill": {
     "duration": 0.114158,
     "end_time": "2024-08-07T01:02:15.652478",
     "exception": false,
     "start_time": "2024-08-07T01:02:15.538320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2592, -2.3711, -2.2918, -2.2507, -2.2794, -2.3193, -2.2831, -2.3106,\n",
       "         -2.3030, -2.3649]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn((28,28))\n",
    "X = X.view(-1,28*28)\n",
    "output = net(X)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f1dcb4",
   "metadata": {
    "papermill": {
     "duration": 0.004118,
     "end_time": "2024-08-07T01:02:15.661253",
     "exception": false,
     "start_time": "2024-08-07T01:02:15.657135",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Calculating loss and specifying the optimizer\n",
    "Now, let's create a loss function to calculate how correct/incorrect our classifications are. Here, we create a scaling score as that metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48b8453a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T01:02:15.672024Z",
     "iopub.status.busy": "2024-08-07T01:02:15.671626Z",
     "iopub.status.idle": "2024-08-07T01:02:15.677769Z",
     "shell.execute_reply": "2024-08-07T01:02:15.676612Z"
    },
    "papermill": {
     "duration": 0.014318,
     "end_time": "2024-08-07T01:02:15.679997",
     "exception": false,
     "start_time": "2024-08-07T01:02:15.665679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73930051",
   "metadata": {
    "papermill": {
     "duration": 0.004383,
     "end_time": "2024-08-07T01:02:15.689152",
     "exception": false,
     "start_time": "2024-08-07T01:02:15.684769",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Running on x epochs\n",
    "\n",
    "Now, let's create a dataloader to load the data in batches from train_dataset. We can set the batch size to 64 and allow data shuffling. We'll then loop 10 times over the batches. X, y = batch unpacks our batches into features X and labels y. net.zero_grad() resets the gradients of all parameters to 0. This is done before each backward pass to prevent gradient accumulation from prior batches.\n",
    "\n",
    "We then reshape our X to 784 to get predictions and calculate negative log likelihood loss between output and true labels. With loss.backward(), we compute the gradient of the loss and then update our network's parameters based on computed gradients in optimizer.step()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46168942",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T01:02:15.700364Z",
     "iopub.status.busy": "2024-08-07T01:02:15.699402Z",
     "iopub.status.idle": "2024-08-07T01:04:50.065724Z",
     "shell.execute_reply": "2024-08-07T01:04:50.064409Z"
    },
    "papermill": {
     "duration": 154.380685,
     "end_time": "2024-08-07T01:04:50.074426",
     "exception": false,
     "start_time": "2024-08-07T01:02:15.693741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.13348448276519775\n",
      "Epoch 2, Loss: 0.16148002445697784\n",
      "Epoch 3, Loss: 0.1771702617406845\n",
      "Epoch 4, Loss: 0.4811420440673828\n",
      "Epoch 5, Loss: 0.121342733502388\n",
      "Epoch 6, Loss: 0.5346405506134033\n",
      "Epoch 7, Loss: 0.20544114708900452\n",
      "Epoch 8, Loss: 0.05130647495388985\n",
      "Epoch 9, Loss: 0.011204591952264309\n",
      "Epoch 10, Loss: 0.4150692820549011\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "for epoch in range(10):  \n",
    "    for batch in train_loader:  \n",
    "        X, y = batch \n",
    "        net.zero_grad()  \n",
    "        output = net(X.view(-1, 784))  \n",
    "        loss = F.nll_loss(output, y) \n",
    "        loss.backward()  \n",
    "        optimizer.step() \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add05b37",
   "metadata": {
    "papermill": {
     "duration": 0.004849,
     "end_time": "2024-08-07T01:04:50.084360",
     "exception": false,
     "start_time": "2024-08-07T01:04:50.079511",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Defining a training loop and making predictions\n",
    "\n",
    "Here, we define our training loop and prediction function. From there, we can train multiple models and ensemble them to then make predictions that we'll submit for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6be7d996",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T01:04:50.096285Z",
     "iopub.status.busy": "2024-08-07T01:04:50.095901Z",
     "iopub.status.idle": "2024-08-07T01:04:56.947021Z",
     "shell.execute_reply": "2024-08-07T01:04:56.945596Z"
    },
    "papermill": {
     "duration": 6.860299,
     "end_time": "2024-08-07T01:04:56.949737",
     "exception": false,
     "start_time": "2024-08-07T01:04:50.089438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions...\n",
      "Submission saved to submission.csv\n",
      "   ImageId  Label\n",
      "0        1      2\n",
      "1        2      0\n",
      "2        3      9\n",
      "3        4      9\n",
      "4        5      3\n"
     ]
    }
   ],
   "source": [
    "def get_predictions(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for X, _ in test_loader:\n",
    "            output = model(X.view(-1, 784))\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            predictions.extend(predicted.numpy())\n",
    "    return predictions\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"Making predictions...\")\n",
    "predictions = get_predictions(net, test_loader)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'ImageId': range(1, len(predictions) + 1),\n",
    "    'Label': predictions\n",
    "})\n",
    "\n",
    "submission_path = 'submission.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"Submission saved to {submission_path}\")\n",
    "\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 861823,
     "sourceId": 3004,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 178.106533,
   "end_time": "2024-08-07T01:04:58.180781",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-07T01:02:00.074248",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
